{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer=\"Writer-Name\"\n",
    "base_url = f\"https://#Some_Website/{writer}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = requests.get(base_url).text\n",
    "soup = BeautifulSoup(source,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol= soup.find(class_='archive-description taxonomy-archive-description').ol\n",
    "main_category = {}\n",
    "\n",
    "for i in ol:\n",
    "    if str(type(i))==\"<class 'bs4.element.NavigableString'>\":\n",
    "        continue\n",
    "    try:\n",
    "        item = i.findChildren()[0]\n",
    "        main_category[item.text] = item['href']\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in main_category.items():\n",
    "    value = \"#_url_of_the_book\" # comment out to see actual url\n",
    "    print(key,'\\t',value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_text import clean_text\n",
    "\n",
    "def extract_text_from_section(text_link):\n",
    "    src = requests.get(text_link).text\n",
    "    soup = BeautifulSoup(src,'lxml')\n",
    "\n",
    "    txt = soup.find('div', id='ftwp-postcontent')\n",
    "    if txt:\n",
    "        pass\n",
    "    else:\n",
    "        txt = soup.find(class_='entry-content entry-content-single')\n",
    "    \n",
    "    if not txt:\n",
    "        txt = soup.find(class_=\"ld-tab-content ld-visible\")\n",
    "\n",
    "\n",
    "    # or use txt.get_text(separator=\"\\n\",strip=True)\n",
    "    extracted_text = txt.text\n",
    "\n",
    "    # For extension purpose, if ever needed\n",
    "    # text = ''\n",
    "    # for e in txt.descendants:\n",
    "    #     if isinstance(e, str):\n",
    "    #         text += e\n",
    "    #     elif e.name == 'br' or e.name == 'p':\n",
    "    #         text += '\\n'\n",
    "    # extracted_text = text\n",
    "\n",
    "    return extracted_text\n",
    "\n",
    "def scrape_all_sections_and_write_file(book_name,sections,parent_directory):\n",
    "    book_name_original = book_name\n",
    "    # book_name_Example = 'ময়ূরকণ্ঠী (১৯৫২)'\n",
    "    book_name = book_name.split(\"(\")[0].strip()\n",
    "    illegals =[\"\\\\\",\"/\", \":\", \"*\", \"?\", \"<\", \">\", \"|\"]\n",
    "    for illegal in illegals:\n",
    "        if illegal in book_name:\n",
    "            book_name = re.sub(illegal,\"-\",book_name)\n",
    "    file_name = os.path.join(parent_directory,f'{book_name}.json')\n",
    "\n",
    "    if os.path.isfile(file_name): \n",
    "        print(\"Book with same name already exists! Renaming book by adding '_2'\")\n",
    "        book_name=book_name+\"_2\"\n",
    "        # update filename with new name\n",
    "        file_name = os.path.join(parent_directory,f'{book_name}.json')\n",
    "    \n",
    "    book_text = \"\"\n",
    "    for text_link in sections.values():\n",
    "        # print(text_link)\n",
    "        txt = extract_text_from_section(text_link)\n",
    "        book_text = book_text + txt + '\\n\\n'\n",
    "    \n",
    "    data = re.split(\"\\n\", book_text)\n",
    "    temp = []\n",
    "    for _data in data:\n",
    "        temp.extend(_data.split(\" \"))\n",
    "    data = temp\n",
    "\n",
    "    wordList = data\n",
    "    # uncomment to apply data cleaning\n",
    "    # wordList = clean_text(data)\n",
    "    \n",
    "    try:\n",
    "        ## TOO SLOW\n",
    "        ## file_name=file_name[:-4]+\"txt\"\n",
    "        ## chunk_size = 1024 * 1024  # 1MB\n",
    "        ## with open(file_name, 'a', encoding='utf-8') as f:\n",
    "        ##     for i in range(0, len(book_text), chunk_size):\n",
    "        ##         f.write(book_text[i:i + chunk_size])\n",
    "\n",
    "        # Save as text (JSON TEXT)\n",
    "        with open(file_name, \"w\", encoding='utf=8') as outfile:\n",
    "            json.dump({book_name_original: book_text}, outfile, ensure_ascii=False)\n",
    "        print(book_name, \"successfully saved\", end='\\n\\n')\n",
    "        \n",
    "        # Save as json \n",
    "        # with open(file_name, \"w\", encoding='utf=8') as outfile:\n",
    "        #     json.dump({book_name_original:wordList}, outfile, ensure_ascii=False)\n",
    "        # print(book_name, \"successfully saved\", end='\\n\\n')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        if os.path.exists(file_name):\n",
    "            os.remove(file_name)\n",
    "\n",
    "def scrape_other_way(lins,book_name,path):\n",
    "    parts={}\n",
    "    for lin in lins:\n",
    "        name = lin.find(class_=\"ld-item-title\").text.strip()\n",
    "        link = lin.attrs[\"href\"]\n",
    "        parts[name] = link\n",
    "    \n",
    "    for key,value in parts.items():\n",
    "        value = \"#_url_of_the_book\" # comment out to see actual url\n",
    "        print(key,'\\t',value)\n",
    "\n",
    "    print(len(parts),\"Sections ...\")\n",
    "    \n",
    "    scrape_all_sections_and_write_file(book_name,parts,path)\n",
    "\n",
    "\n",
    "def go_inside_category(book_name,book_link,flag=0):\n",
    "    \n",
    "    # print(category_name,category_link)\n",
    "\n",
    "    parent_directory = f\"G:\\_Somikoron\\Web Scraping\\{writer}-JSON\" #create this folder yourself\n",
    "    directory = book_name\n",
    "    # delete the following line to have subfolders for each file\n",
    "    directory=\"\"\n",
    "    path = os.path.join(parent_directory,directory)\n",
    "    print(book_name) #path\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    src = requests.get(book_link).text\n",
    "    soup2 = BeautifulSoup(src,'lxml')\n",
    "\n",
    "    # new\n",
    "    lins = soup2.findAll('a',class_=\"ld-item-name ld-primary-color-hover\")\n",
    "    if lins:\n",
    "        scrape_other_way(lins,book_name,path)\n",
    "        return\n",
    "    \n",
    "    ol2= soup2.find_all(class_='entry-title-link')\n",
    "\n",
    "    sections = {}\n",
    "    cnt =1\n",
    "    for i in ol2:\n",
    "        try:\n",
    "            item = i\n",
    "            s_name = item.text\n",
    "            if s_name in sections:\n",
    "                cnt += 1\n",
    "                s_name = s_name + f\"-{cnt}\"\n",
    "            else:\n",
    "                cnt= 1\n",
    "            sections[s_name] = item['href']\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    pagination = soup2.find(attrs ={\"role\" : \"navigation\"})\n",
    "    if pagination and pagination.find(class_=\"active\") and pagination.find(class_=\"pagination-next\"):\n",
    "        next_url=pagination.find(class_=\"pagination-next\").find('a')['href']\n",
    "        new_sections=go_inside_category(book_name,next_url,flag=1)\n",
    "        for key in new_sections.keys():\n",
    "            sections[key]=new_sections[key]\n",
    "    \n",
    "    if flag==1:\n",
    "        return sections\n",
    "\n",
    "    for key,value in sections.items():\n",
    "        value = \"#_url_of_the_book\" # comment out to see actual url\n",
    "        print(key,'\\t',value)\n",
    "\n",
    "    print(len(sections),\"Sections ...\")\n",
    "\n",
    "    scrape_all_sections_and_write_file(book_name,sections,path)\n",
    "        \n",
    "        #uncomment to work only one book\n",
    "        # break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category_name, category_link in main_category.items():\n",
    "\n",
    "    try:\n",
    "        go_inside_category(category_name,category_link)\n",
    "    except Exception as e:\n",
    "        print(f\"{category_name} was not scraped, {e}\")\n",
    "\n",
    "    # break #break after 1 iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scrape only one book from main_category\n",
    "# print(main_category.keys(),\"\\n\\n\")\n",
    "\n",
    "# book_name = \"কাঁদো নদী কাঁদো\"\n",
    "# if book_name in main_category:\n",
    "#     try:\n",
    "#         go_inside_category(book_name,book_link=main_category[book_name])\n",
    "#     except Exception as e:\n",
    "#         print(f\"{book_name} was not scraped, {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
