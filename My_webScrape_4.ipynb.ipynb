{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from clean_text import clean_text\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_obj = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_section(text_link):\n",
    "    src = session_obj.get(text_link).text\n",
    "    soup = BeautifulSoup(src,'lxml')\n",
    "\n",
    "    txt = soup.find('div', id='ftwp-postcontent')\n",
    "    if txt:\n",
    "        pass\n",
    "    else:\n",
    "        txt = soup.find(class_='entry-content entry-content-single')\n",
    "    \n",
    "    if not txt:\n",
    "        txt = soup.find(class_=\"ld-tab-content ld-visible\")\n",
    "\n",
    "\n",
    "    # or use txt.get_text(separator=\"\\n\",strip=True)\n",
    "    extracted_text = txt.text\n",
    "\n",
    "    # For extension purpose, if ever needed\n",
    "    # text = ''\n",
    "    # for e in txt.descendants:\n",
    "    #     if isinstance(e, str):\n",
    "    #         text += e\n",
    "    #     elif e.name == 'br' or e.name == 'p':\n",
    "    #         text += '\\n'\n",
    "    # extracted_text = text\n",
    "\n",
    "    return extracted_text\n",
    "\n",
    "\n",
    "def scrape_all_sections_and_write_file(book_name,sections,parent_directory):\n",
    "    book_name_original = book_name\n",
    "    # book_name_Example = 'ময়ূরকণ্ঠী (১৯৫২)'\n",
    "    book_name = book_name.split(\"(\")[0].strip()\n",
    "    illegals =[\"\\\\\",\"/\", \":\", \"*\", \"?\", \"<\", \">\", \"|\"]\n",
    "    for illegal in illegals:\n",
    "        if illegal in book_name:\n",
    "            book_name = re.sub(illegal,\"-\",book_name)\n",
    "    file_name = os.path.join(parent_directory,f'{book_name}.json')\n",
    "\n",
    "    if os.path.isfile(file_name): \n",
    "        print(\"Book with same name already exists! Renaming book by adding '_2'\")\n",
    "        book_name=book_name+\"_2\"\n",
    "        # update filename with new name\n",
    "        file_name = os.path.join(parent_directory,f'{book_name}.json')\n",
    "    \n",
    "    # # using for loop\n",
    "    # book_text = \"\"\n",
    "    # for text_link in sections.values():\n",
    "    #     # print(text_link)\n",
    "    #     txt = extract_text_from_section(text_link)\n",
    "    #     book_text = book_text + txt + '\\n\\n'\n",
    "    \n",
    "    # using multithreading\n",
    "    text_links = list(sections.values())\n",
    "    output_list = None\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        output_list = list(executor.map(extract_text_from_section, text_links))\n",
    "    book_text = '\\n\\n'.join(output_list)\n",
    "    del output_list\n",
    "    \n",
    "    # data = re.split(\"\\n\", book_text)\n",
    "    # temp = []\n",
    "    # for _data in data:\n",
    "    #     temp.extend(_data.split(\" \"))\n",
    "    # data = temp\n",
    "\n",
    "    # wordList = data\n",
    "    # uncomment to apply data cleaning\n",
    "    # wordList = clean_text(data)\n",
    "    \n",
    "    try:\n",
    "        ## TOO SLOW\n",
    "        ## file_name=file_name[:-4]+\"txt\"\n",
    "        ## chunk_size = 1024 * 1024  # 1MB\n",
    "        ## with open(file_name, 'a', encoding='utf-8') as f:\n",
    "        ##     for i in range(0, len(book_text), chunk_size):\n",
    "        ##         f.write(book_text[i:i + chunk_size])\n",
    "\n",
    "        # Save as text (JSON TEXT)\n",
    "        with open(file_name, \"w\", encoding='utf=8') as outfile:\n",
    "            json.dump({book_name_original: book_text}, outfile, ensure_ascii=False)\n",
    "        print(book_name, \"successfully saved\", end='\\n\\n')\n",
    "        \n",
    "        # Save as json \n",
    "        # with open(file_name, \"w\", encoding='utf=8') as outfile:\n",
    "        #     json.dump({book_name_original:wordList}, outfile, ensure_ascii=False)\n",
    "        # print(book_name, \"successfully saved\", end='\\n\\n')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        if os.path.exists(file_name):\n",
    "            os.remove(file_name)\n",
    "\n",
    "\n",
    "def scrape_other_way(lins,book_name,path):\n",
    "    parts={}\n",
    "    for lin in lins:\n",
    "        name = lin.find(class_=\"ld-item-title\").text.strip()\n",
    "        link = lin.attrs[\"href\"]\n",
    "        parts[name] = link\n",
    "    \n",
    "    for key,value in parts.items():\n",
    "        value = \"#_url_of_the_book\" # comment out to see actual url\n",
    "        print(key,'\\t',value)\n",
    "\n",
    "    print(len(parts),\"Sections ...\")\n",
    "    \n",
    "    scrape_all_sections_and_write_file(book_name,parts,path)\n",
    "\n",
    "\n",
    "def go_inside_category(book_name,book_link,flag=0, writer=None):\n",
    "    # print(category_name,category_link)\n",
    "\n",
    "    parent_directory = f\"G:\\_Somikoron\\Web Scraping\\{writer}-TEXT\" #create this folder yourself\n",
    "    directory = book_name\n",
    "    # delete the following line to have subfolders for each file\n",
    "    directory=\"\"\n",
    "    path = os.path.join(parent_directory,directory)\n",
    "    print(book_name) #path\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    src = session_obj.get(book_link).text\n",
    "    soup2 = BeautifulSoup(src,'lxml')\n",
    "\n",
    "    # new\n",
    "    lins = soup2.findAll('a',class_=\"ld-item-name ld-primary-color-hover\")\n",
    "    if lins:\n",
    "        scrape_other_way(lins,book_name,path)\n",
    "        return\n",
    "    \n",
    "    ol2= soup2.find_all(class_='entry-title-link')\n",
    "\n",
    "    sections = {}\n",
    "    cnt =1\n",
    "    for i in ol2:\n",
    "        try:\n",
    "            item = i\n",
    "            s_name = item.text\n",
    "            if s_name in sections:\n",
    "                cnt += 1\n",
    "                s_name = s_name + f\"-{cnt}\"\n",
    "            else:\n",
    "                cnt= 1\n",
    "            sections[s_name] = item['href']\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    pagination = soup2.find(attrs ={\"role\" : \"navigation\"})\n",
    "    if pagination and pagination.find(class_=\"active\") and pagination.find(class_=\"pagination-next\"):\n",
    "        next_url=pagination.find(class_=\"pagination-next\").find('a')['href']\n",
    "        new_sections=go_inside_category(book_name,next_url,flag=1, writer=writer)\n",
    "        for key in new_sections.keys():\n",
    "            sections[key]=new_sections[key]\n",
    "    \n",
    "    if flag==1:\n",
    "        return sections\n",
    "\n",
    "    for key,value in sections.items():\n",
    "        value = \"#_url_of_the_book\" # comment out to see actual url\n",
    "        print(key,'\\t',value)\n",
    "\n",
    "    print(len(sections),\"Sections ...\")\n",
    "\n",
    "    scrape_all_sections_and_write_file(book_name,sections,path)\n",
    "        \n",
    "        #uncomment to work only one book\n",
    "        # break \n",
    "\n",
    "\n",
    "def scrape_all_writers(writers):\n",
    "    for writer in writers:\n",
    "        base_urls = [f\"https://#website/link1/{writer}\", f\"https://#website/link2/{writer}\"]\n",
    "        # base_urls=[base_urls[1]]\n",
    "        main_category = {}\n",
    "\n",
    "        not_found = 0\n",
    "        for cnt,base_url in enumerate(base_urls,1):\n",
    "            source = session_obj.get(base_url).text\n",
    "            soup = BeautifulSoup(source,'lxml')\n",
    "\n",
    "            if not soup:\n",
    "                print(\"Link\", cnt, \"doesn't exist\")\n",
    "                continue\n",
    "\n",
    "            ol= soup.find(class_='archive-description taxonomy-archive-description')\n",
    "            if ol: ol= ol.ol\n",
    "            etls = soup.find_all(\"a\", class_=\"entry-title-link\")\n",
    "\n",
    "            if (not ol) and etls:\n",
    "                for etl in etls:\n",
    "                    main_category[etl.text]=etl.attrs[\"href\"]        \n",
    "            \n",
    "            if not ol and not etls:\n",
    "                    not_found += 1\n",
    "\n",
    "            if not (ol or etls):\n",
    "                print(\"link\", cnt, \"doesn't exist\")\n",
    "                continue\n",
    "            \n",
    "            if not ol:\n",
    "                continue\n",
    "            \n",
    "            for i in ol:\n",
    "                if str(type(i))==\"<class 'bs4.element.NavigableString'>\":\n",
    "                    continue\n",
    "                try:\n",
    "                    item = i.findChildren()[0]\n",
    "                    main_category[item.text] = item['href']\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "        \n",
    "        if not_found == 2:\n",
    "            print(writer, \"was not scraped, find correct link\")\n",
    "\n",
    "        for key,value in main_category.items():\n",
    "            value = \"#_url_of_the_book\" # comment out to see actual url\n",
    "            print(key,'\\t',value)\n",
    "        \n",
    "        for category_name, category_link in main_category.items():\n",
    "            try:\n",
    "                go_inside_category(category_name,category_link, writer=writer)\n",
    "            except Exception as e:\n",
    "                print(f\"{category_name} was not scraped, {e}\")\n",
    "\n",
    "            # break #break after 1 iteration\n",
    "    \n",
    "    locals().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scrape only one book\n",
    "# book_name = \"\"\n",
    "# book_link = \"\"\n",
    "# writer = \"\"\n",
    "# try:\n",
    "#     go_inside_category(book_name,book_link=book_link, writer=writer)\n",
    "#     # scrape_all_writers([writer])\n",
    "# except Exception as e:\n",
    "#     print(f\"{writer} was not scraped, {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "with open(\"./kallol_writers_scraped.json\", encoding=\"UTF-8\") as fp:\n",
    "    d = json.load(fp)\n",
    "writers = list(d.keys())\n",
    "writers = [re.sub(\" \", \"-\", i) for i in writers]\n",
    "# writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already scraped from 0 to 260, start from 261\n",
    "i= 260\n",
    "writers_list = writers[i:i+1]\n",
    "# writers_list = [\"\"]\n",
    "scrape_all_writers(writers_list)\n",
    "writers[i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
